#!/usr/bin/env python3
"""
å®éªŒ9ï¼šéŸ³é¢‘é•¿åº¦ Scaling å®éªŒ

ç›®æ ‡ï¼š
æµ‹é‡ä¸åŒéŸ³é¢‘é•¿åº¦ï¼ˆ1s/3s/6s/10s ç­‰ï¼‰å¯¹ Audio Encoder å»¶è¿Ÿçš„å½±å“
éªŒè¯å‡è®¾ï¼šAudio Encoder è€—æ—¶ä¸åºåˆ—é•¿åº¦ï¼ˆæ¥è¿‘ï¼‰çº¿æ€§ç›¸å…³

åŸºäº exp7 ç»“æ„ï¼Œèšç„¦äºéŸ³é¢‘é•¿åº¦è¿™ä¸€ä¸ªå˜é‡
"""

from __future__ import annotations
import argparse
import gc
import json
import os
import sys
import time
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')

# æ·»åŠ é¡¹ç›®è·¯å¾„
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_DIR = os.path.dirname(SCRIPT_DIR)
if PROJECT_DIR not in sys.path:
    sys.path.insert(0, PROJECT_DIR)

import common as C
import profiling_utils as P


# ============ Audio Encoder è®¡æ—¶ Hook ============
# Replaced by profiling_utils.ModuleCudaEventTimer but keeping alias for compatibility if needed, 
# or just using P.ModuleCudaEventTimer directly.
# The original AudioEncoderTimer is very similar to ModuleCudaEventTimer but specialized for Audio Tower.
# Let's use P.ModuleCudaEventTimer and adapt.

class AudioEncoderTimer(P.ModuleCudaEventTimer):
    """ä¸“é—¨æµ‹é‡ Audio Encoder è€—æ—¶çš„ Hook (Wrapper around ModuleCudaEventTimer)"""
    def register(self, model):
        # Specific registration for audio tower
        super().register(model.thinker.audio_tower)

# ============ éŸ³é¢‘å¤„ç† ============


# ============ éŸ³é¢‘å¤„ç† ============

def truncate_audio(audio: np.ndarray, target_seconds: float, sample_rate: int = 16000) -> np.ndarray:
    """æˆªæ–­éŸ³é¢‘åˆ°æŒ‡å®šç§’æ•°"""
    target_samples = int(target_seconds * sample_rate)
    if len(audio) > target_samples:
        return audio[:target_samples]
    if len(audio) < target_samples:
        pad = np.zeros((target_samples - len(audio),), dtype=audio.dtype)
        return np.concatenate([audio, pad], axis=0)
    return audio


def get_audio_from_video(video_path: str) -> Tuple[np.ndarray, int]:
    """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘"""
    import subprocess
    import tempfile
    import soundfile as sf
    
    # ä½¿ç”¨ ffmpeg æå–éŸ³é¢‘
    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as f:
        temp_path = f.name
    
    try:
        cmd = [
            'ffmpeg', '-y', '-i', video_path,
            '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1',
            temp_path
        ]
        subprocess.run(cmd, capture_output=True, check=True)
        audio, sr = sf.read(temp_path)
        return audio.astype(np.float32), sr
    finally:
        if os.path.exists(temp_path):
            os.remove(temp_path)


# ============ æ¨ç†å‡½æ•° ============

def run_single_audio_test(
    model,
    proc,
    video_path: str,
    audio_seconds: float,
    fe,
    timer: AudioEncoderTimer,
) -> Dict:
    """å¯¹å•ä¸ªéŸ³é¢‘é•¿åº¦è¿è¡Œä¸€æ¬¡æµ‹è¯•"""
    from qwen_omni_utils import process_mm_info
    
    gc.collect()
    torch.cuda.empty_cache()
    
    # 1. æå–è§†é¢‘å¸§å’ŒéŸ³é¢‘
    conversation = [{"role": "user", "content": [
        {"type": "video", "video": video_path},
        {"type": "text", "text": "Describe what you see and hear."}
    ]}]
    
    audios, images, videos = process_mm_info(conversation, use_audio_in_video=True)
    
    if not audios:
        raise ValueError("No audio extracted from video")
    
    # 2. æˆªæ–­éŸ³é¢‘
    original_audio = audios[0]
    original_duration = len(original_audio) / 16000
    truncated_audio = truncate_audio(original_audio, audio_seconds)
    actual_duration = len(truncated_audio) / 16000
    
    # 3. å‡†å¤‡è¾“å…¥
    text = proc.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)
    inputs = proc(text=text, videos=videos, return_tensors="pt", padding=True).to(model.device)
    
    # 4. éŸ³é¢‘ç‰¹å¾æå–ï¼ˆFFT + Melï¼‰
    t_fft_start = time.perf_counter()
    af = fe(
        truncated_audio,
        sampling_rate=16000,
        return_tensors='pt',
        padding='do_not_pad',
        truncation=False,
    )
    inputs['input_features'] = af['input_features'].to(model.device, dtype=torch.bfloat16)
    inputs['feature_attention_mask'] = torch.ones(
        (1, af['input_features'].shape[2]), device=model.device, dtype=torch.long
    )
    if torch.cuda.is_available():
        torch.cuda.synchronize(device=model.device)
    fft_mel_ms = (time.perf_counter() - t_fft_start) * 1000
    
    # è®°å½• mel å¸§æ•°
    mel_frames = af['input_features'].shape[2]
    
    # 5. è¿è¡Œ generateï¼ˆåªç”Ÿæˆ 1 token ä»¥æµ‹ TTFTï¼‰
    timer.clear()  # æ¸…é™¤ä¹‹å‰çš„è®¡æ—¶
    
    t_gen_start = time.perf_counter()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=1,
            do_sample=False,
            return_audio=False,
        )
    if torch.cuda.is_available():
        devices = set()
        devices.add(model.device)
        try:
            devices.add(next(model.thinker.audio_tower.parameters()).device)
        except StopIteration:
            pass
        try:
            devices.add(next(model.thinker.visual.parameters()).device)
        except StopIteration:
            pass
        for d in devices:
            if isinstance(d, torch.device) and d.type == "cuda":
                torch.cuda.synchronize(device=d)
    ttft_ms = (time.perf_counter() - t_gen_start) * 1000
    
    audio_encoder_ms = timer.get_last()
    audio_tower_in_frames = None
    if timer.last_input_shape is not None and len(timer.last_input_shape) >= 2:
        audio_tower_in_frames = int(timer.last_input_shape[-1])
    audio_tower_input_shape = str(timer.last_input_shape) if timer.last_input_shape is not None else None
    
    return {
        "original_duration_s": original_duration,
        "target_duration_s": audio_seconds,
        "actual_duration_s": actual_duration,
        "mel_frames": mel_frames,
        "audio_tower_in_frames": audio_tower_in_frames,
        "audio_tower_input_shape": audio_tower_input_shape,
        "fft_mel_ms": fft_mel_ms,
        "audio_encoder_ms": audio_encoder_ms,
        "ttft_ms": ttft_ms,
    }


# ============ å¯è§†åŒ– ============

def plot_scaling_curve(results_df: pd.DataFrame, output_path: str):
    """ç»˜åˆ¶éŸ³é¢‘é•¿åº¦ vs encoder å»¶è¿Ÿæ›²çº¿"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # æŒ‰éŸ³é¢‘é•¿åº¦åˆ†ç»„ç»Ÿè®¡
    grouped = results_df.groupby("target_duration_s").agg({
        "audio_encoder_ms": ["mean", "std"],
        "fft_mel_ms": ["mean", "std"],
        "mel_frames": "mean",
        "ttft_ms": ["mean", "std"],
    }).reset_index()
    
    # å±•å¹³åˆ—å
    grouped.columns = [
        "duration_s",
        "encoder_mean", "encoder_std",
        "fft_mean", "fft_std",
        "mel_frames",
        "ttft_mean", "ttft_std",
    ]
    
    x = grouped["duration_s"].values
    
    # å›¾1ï¼šéŸ³é¢‘é•¿åº¦ vs Audio Encoder å»¶è¿Ÿ
    axes[0].errorbar(x, grouped["encoder_mean"], yerr=grouped["encoder_std"],
                     marker='o', capsize=5, linewidth=2, markersize=8, color='#2196F3')
    axes[0].set_xlabel("Audio Duration (seconds)", fontsize=12)
    axes[0].set_ylabel("Audio Encoder Time (ms)", fontsize=12)
    axes[0].set_title("Audio Duration vs Encoder Latency", fontsize=14)
    axes[0].grid(True, alpha=0.3)
    
    # æ·»åŠ çº¿æ€§æ‹Ÿåˆ
    if len(x) >= 2:
        z = np.polyfit(x, grouped["encoder_mean"], 1)
        p = np.poly1d(z)
        x_fit = np.linspace(min(x), max(x), 100)
        axes[0].plot(x_fit, p(x_fit), '--', color='red', alpha=0.7, 
                     label=f'Linear fit: {z[0]:.1f}ms/s')
        axes[0].legend()
    
    # å›¾2ï¼šMel å¸§æ•° vs Encoder å»¶è¿Ÿ
    axes[1].scatter(grouped["mel_frames"], grouped["encoder_mean"],
                    s=100, c='#4CAF50', edgecolors='black', linewidths=1)
    axes[1].set_xlabel("Mel Frames (sequence length)", fontsize=12)
    axes[1].set_ylabel("Audio Encoder Time (ms)", fontsize=12)
    axes[1].set_title("Mel Frames vs Encoder Latency", fontsize=14)
    axes[1].grid(True, alpha=0.3)
    
    # æ·»åŠ æ ‡æ³¨
    for i, row in grouped.iterrows():
        axes[1].annotate(f'{row["duration_s"]:.0f}s',
                        (row["mel_frames"], row["encoder_mean"]),
                        textcoords="offset points", xytext=(5, 5), fontsize=9)
    
    # å›¾3ï¼šå„é˜¶æ®µæ—¶é—´å †å æ¡å½¢å›¾
    bar_width = 0.6
    bars1 = axes[2].bar(x, grouped["fft_mean"], bar_width, label='FFT+Mel', color='#FFC107')
    bars2 = axes[2].bar(x, grouped["encoder_mean"], bar_width, bottom=grouped["fft_mean"],
                        label='Audio Encoder', color='#2196F3')
    
    axes[2].set_xlabel("Audio Duration (seconds)", fontsize=12)
    axes[2].set_ylabel("Time (ms)", fontsize=12)
    axes[2].set_title("Audio Processing Time Breakdown", fontsize=14)
    axes[2].legend()
    axes[2].grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ æ•°å€¼æ ‡æ³¨
    for i, (d, enc, fft) in enumerate(zip(x, grouped["encoder_mean"], grouped["fft_mean"])):
        total = enc + fft
        axes[2].text(d, total + 10, f'{total:.0f}ms', ha='center', fontsize=9, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"ğŸ“Š Scaling æ›²çº¿å›¾å·²ä¿å­˜: {output_path}")


def plot_encoder_vs_duration_detail(results_df: pd.DataFrame, output_path: str):
    """ç»˜åˆ¶è¯¦ç»†çš„ encoder å»¶è¿Ÿåˆ†æå›¾"""
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # æ•£ç‚¹å›¾ï¼šæ¯ä¸ªæ ·æœ¬ä¸€ä¸ªç‚¹
    scatter = ax.scatter(
        results_df["actual_duration_s"],
        results_df["audio_encoder_ms"],
        c=results_df["target_duration_s"],
        cmap='viridis',
        s=60,
        alpha=0.7,
        edgecolors='black',
        linewidths=0.5,
    )
    
    ax.set_xlabel("Actual Audio Duration (seconds)", fontsize=12)
    ax.set_ylabel("Audio Encoder Time (ms)", fontsize=12)
    ax.set_title("Audio Encoder Latency vs Duration (All Samples)", fontsize=14)
    ax.grid(True, alpha=0.3)
    
    # é¢œè‰²æ¡
    cbar = plt.colorbar(scatter)
    cbar.set_label("Target Duration (s)")
    
    # çº¿æ€§æ‹Ÿåˆ
    x = results_df["actual_duration_s"].values
    y = results_df["audio_encoder_ms"].values
    if len(x) >= 2:
        z = np.polyfit(x, y, 1)
        p = np.poly1d(z)
        x_fit = np.linspace(min(x), max(x), 100)
        ax.plot(x_fit, p(x_fit), '--', color='red', linewidth=2,
                label=f'Linear: y = {z[0]:.1f}x + {z[1]:.1f}')
        
        # RÂ² å€¼
        y_pred = p(x)
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
        ax.legend(title=f'RÂ² = {r2:.3f}')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"ğŸ“Š è¯¦ç»†åˆ†æå›¾å·²ä¿å­˜: {output_path}")


def plot_encoder_vs_mel_frames_detail(results_df: pd.DataFrame, output_path: str):
    fig, ax = plt.subplots(figsize=(10, 6))

    scatter = ax.scatter(
        results_df["mel_frames"],
        results_df["audio_encoder_ms"],
        c=results_df["target_duration_s"],
        cmap='viridis',
        s=60,
        alpha=0.7,
        edgecolors='black',
        linewidths=0.5,
    )

    ax.set_xlabel("Mel Frames (sequence length)", fontsize=12)
    ax.set_ylabel("Audio Encoder Time (ms)", fontsize=12)
    ax.set_title("Audio Encoder Latency vs Mel Frames (All Samples)", fontsize=14)
    ax.grid(True, alpha=0.3)

    cbar = plt.colorbar(scatter)
    cbar.set_label("Target Duration (s)")

    x = results_df["mel_frames"].values
    y = results_df["audio_encoder_ms"].values
    if len(x) >= 2:
        z = np.polyfit(x, y, 1)
        p = np.poly1d(z)
        x_fit = np.linspace(float(np.min(x)), float(np.max(x)), 100)
        ax.plot(x_fit, p(x_fit), '--', color='red', linewidth=2,
                label=f'Linear: y = {z[0]:.6f}x + {z[1]:.1f}')
        y_pred = p(x)
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
        ax.legend(title=f'RÂ² = {r2:.3f}')

    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"ğŸ“Š Mel Frames è¯¦ç»†åˆ†æå›¾å·²ä¿å­˜: {output_path}")


# ============ ä¸»å‡½æ•° ============

def main():
    parser = argparse.ArgumentParser(description="éŸ³é¢‘é•¿åº¦ Scaling å®éªŒ")
    parser.add_argument("--model", default="/root/autodl-tmp/Qwen2.5-Omni-7B", help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--data", default="/root/autodl-tmp/data/MSRVTT_subset/manifest.csv", help="æ•°æ® manifest")
    parser.add_argument("--out-dir", default="/root/autodl-tmp/results/exp9", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--audio-lengths", default="1,3,10,30,60,120,180,240,300", help="è¦æµ‹è¯•çš„éŸ³é¢‘é•¿åº¦ï¼ˆç§’ï¼‰ï¼Œé€—å·åˆ†éš”")
    parser.add_argument("--n-samples", type=int, default=5, help="æ¯ä¸ªé•¿åº¦æµ‹è¯•çš„æ ·æœ¬æ•°")
    parser.add_argument("--warmup", type=int, default=2, help="é¢„çƒ­æ¬¡æ•°")
    args = parser.parse_args()
    
    # è§£æéŸ³é¢‘é•¿åº¦
    audio_lengths = [float(x.strip()) for x in args.audio_lengths.split(",")]
    
    os.makedirs(args.out_dir, exist_ok=True)
    
    print("=" * 70)
    print("ğŸ”Š å®éªŒ9ï¼šéŸ³é¢‘é•¿åº¦ Scaling å®éªŒ")
    print("=" * 70)
    print(f"æ¨¡å‹: {args.model}")
    print(f"æ•°æ®: {args.data}")
    print(f"æµ‹è¯•éŸ³é¢‘é•¿åº¦: {audio_lengths} ç§’")
    print(f"æ¯ä¸ªé•¿åº¦æ ·æœ¬æ•°: {args.n_samples}")
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ”„ åŠ è½½æ¨¡å‹...")
    model, proc = C.load_qwen25_omni(args.model, "bf16")
    
    # åˆ›å»º WhisperFeatureExtractor
    from transformers import WhisperFeatureExtractor
    fe = WhisperFeatureExtractor.from_pretrained(args.model)
    print("âœ… å·²åŠ è½½ WhisperFeatureExtractor")
    
    # æ³¨å†Œ Audio Encoder è®¡æ—¶ Hook
    timer = AudioEncoderTimer()
    timer.register(model)
    print("âœ… å·²æ³¨å†Œ Audio Encoder è®¡æ—¶ Hook")

    try:
        audio_dev = next(model.thinker.audio_tower.parameters()).device
    except StopIteration:
        audio_dev = None
    try:
        visual_dev = next(model.thinker.visual.parameters()).device
    except StopIteration:
        visual_dev = None
    print(f"Device placement: model={model.device}, audio_tower={audio_dev}, visual={visual_dev}")
    
    # åŠ è½½æ•°æ®
    print("\nğŸ”„ åŠ è½½æ•°æ®...")
    if not os.path.exists(args.data):
        print(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data}")
        return
    
    df = pd.read_csv(args.data)
    video_paths = [p for p in df["video_path"].tolist() if os.path.exists(p)]
    print(f"  æ‰¾åˆ° {len(video_paths)} ä¸ªæœ‰æ•ˆè§†é¢‘")
    
    if len(video_paths) < args.n_samples + args.warmup:
        print(f"âš ï¸ è§†é¢‘æ•°é‡ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {args.n_samples + args.warmup} ä¸ª")
        return
    
    # é¢„çƒ­
    print(f"\nğŸ”¥ é¢„çƒ­ ({args.warmup} æ¬¡)...")
    for i in range(args.warmup):
        try:
            run_single_audio_test(model, proc, video_paths[i], 3.0, fe, timer)
            print(f"  é¢„çƒ­ {i+1}/{args.warmup} å®Œæˆ")
        except Exception as e:
            print(f"  é¢„çƒ­ {i+1} å¤±è´¥: {e}")
    
    timer.clear()
    gc.collect()
    torch.cuda.empty_cache()
    
    # æ­£å¼æµ‹è¯•
    print(f"\nğŸ§ª å¼€å§‹ Scaling æµ‹è¯•...")
    results = []
    test_videos = video_paths[args.warmup:args.warmup + args.n_samples]
    
    total_tests = len(audio_lengths) * len(test_videos)
    test_count = 0
    
    for audio_len in audio_lengths:
        print(f"\n--- æµ‹è¯•éŸ³é¢‘é•¿åº¦: {audio_len} ç§’ ---")
        
        for i, video_path in enumerate(test_videos):
            test_count += 1
            print(f"  [{test_count}/{total_tests}] æ ·æœ¬ {i+1}/{len(test_videos)}", end=" ")
            
            try:
                result = run_single_audio_test(model, proc, video_path, audio_len, fe, timer)
                result["video_path"] = os.path.basename(video_path)
                results.append(result)
                
                print(f"âœ“ mel={result['mel_frames']}, encoder={result['audio_encoder_ms']:.0f}ms")
                
            except Exception as e:
                print(f"âœ— å¤±è´¥: {e}")
            
            gc.collect()
            torch.cuda.empty_cache()
    
    # ä¿å­˜ç»“æœ
    print("\nğŸ“ ä¿å­˜ç»“æœ...")
    
    results_df = pd.DataFrame(results)
    results_df.to_csv(os.path.join(args.out_dir, "scaling_results.csv"), index=False)
    
    with open(os.path.join(args.out_dir, "scaling_results.json"), "w") as f:
        json.dump(results, f, indent=2)
    
    # ç»Ÿè®¡ä¸å¯è§†åŒ–
    if results:
        print("\nğŸ“Š ç»˜åˆ¶å›¾è¡¨...")
        plot_scaling_curve(results_df, os.path.join(args.out_dir, "scaling_curve.png"))
        plot_encoder_vs_duration_detail(results_df, os.path.join(args.out_dir, "encoder_detail.png"))
        plot_encoder_vs_mel_frames_detail(results_df, os.path.join(args.out_dir, "mel_frames_detail.png"))
        
        # æ‰“å°ç»Ÿè®¡è¡¨æ ¼
        print("\n" + "=" * 70)
        print("ğŸ“Š Scaling å®éªŒç»“æœ")
        print("=" * 70)
        
        agg_cfg = {
            "mel_frames": "mean",
            "fft_mel_ms": ["mean", "std"],
            "audio_encoder_ms": ["mean", "std"],
            "ttft_ms": ["mean", "std"],
        }
        if "audio_tower_in_frames" in results_df.columns:
            agg_cfg["audio_tower_in_frames"] = "mean"
        grouped = results_df.groupby("target_duration_s").agg(agg_cfg).reset_index()
        
        grouped.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col 
                           for col in grouped.columns.values]
        
        has_audio_tower_frames = "audio_tower_in_frames_mean" in grouped.columns
        if has_audio_tower_frames:
            print(f"\n{'Duration(s)':<12} {'Mel Frames':<12} {'Tower Frames':<12} {'FFT+Mel(ms)':<15} {'Encoder(ms)':<18} {'TTFT(ms)':<15}")
            print("-" * 86)
        else:
            print(f"\n{'Duration(s)':<12} {'Mel Frames':<12} {'FFT+Mel(ms)':<15} {'Encoder(ms)':<18} {'TTFT(ms)':<15}")
            print("-" * 72)
        
        for _, row in grouped.iterrows():
            dur = row["target_duration_s"]
            mel = row["mel_frames_mean"]
            tower_frames = row["audio_tower_in_frames_mean"] if has_audio_tower_frames else None
            fft_mean = row["fft_mel_ms_mean"]
            fft_std = row["fft_mel_ms_std"]
            enc_mean = row["audio_encoder_ms_mean"]
            enc_std = row["audio_encoder_ms_std"]
            ttft_mean = row["ttft_ms_mean"]
            ttft_std = row["ttft_ms_std"]
            
            if has_audio_tower_frames:
                tf = float(tower_frames) if tower_frames is not None and not pd.isna(tower_frames) else float('nan')
                print(f"{dur:<12.0f} {mel:<12.0f} {tf:<12.0f} {fft_mean:>6.1f}Â±{fft_std:<6.1f} {enc_mean:>6.1f}Â±{enc_std:<9.1f} {ttft_mean:>6.1f}Â±{ttft_std:<6.1f}")
            else:
                print(f"{dur:<12.0f} {mel:<12.0f} {fft_mean:>6.1f}Â±{fft_std:<6.1f} {enc_mean:>6.1f}Â±{enc_std:<9.1f} {ttft_mean:>6.1f}Â±{ttft_std:<6.1f}")
        
        # è®¡ç®— scaling ç³»æ•°
        x = results_df["actual_duration_s"].values
        y = results_df["audio_encoder_ms"].values
        if len(x) >= 2:
            z = np.polyfit(x, y, 1)
            print(f"\nğŸ“ˆ Scaling åˆ†æ:")
            print(f"  çº¿æ€§æ‹Ÿåˆ: encoder_ms = {z[0]:.1f} Ã— duration_s + {z[1]:.1f}")
            print(f"  æ¯å¢åŠ  1 ç§’éŸ³é¢‘ï¼Œencoder å»¶è¿Ÿå¢åŠ çº¦ {z[0]:.1f} ms")
            
            # RÂ² å€¼
            p = np.poly1d(z)
            y_pred = p(x)
            ss_res = np.sum((y - y_pred) ** 2)
            ss_tot = np.sum((y - np.mean(y)) ** 2)
            r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
            print(f"  RÂ² = {r2:.4f} (è¶Šæ¥è¿‘ 1 è¡¨ç¤ºçº¿æ€§å…³ç³»è¶Šå¼º)")

        x_mel = results_df["mel_frames"].values
        if len(x_mel) >= 2:
            z_mel = np.polyfit(x_mel, y, 1)
            print(f"\nğŸ“ˆ Scaling (mel_frames) åˆ†æ:")
            print(f"  çº¿æ€§æ‹Ÿåˆ: encoder_ms = {z_mel[0]:.6f} Ã— mel_frames + {z_mel[1]:.1f}")
            print(f"  æ¯å¢åŠ  1000 mel framesï¼Œencoder å»¶è¿Ÿå¢åŠ çº¦ {z_mel[0] * 1000:.1f} ms")
            p_mel = np.poly1d(z_mel)
            y_pred_mel = p_mel(x_mel)
            ss_res_mel = np.sum((y - y_pred_mel) ** 2)
            ss_tot_mel = np.sum((y - np.mean(y)) ** 2)
            r2_mel = 1 - (ss_res_mel / ss_tot_mel) if ss_tot_mel > 0 else 0
            print(f"  RÂ² = {r2_mel:.4f} (è¶Šæ¥è¿‘ 1 è¡¨ç¤ºçº¿æ€§å…³ç³»è¶Šå¼º)")
    
    else:
        print("\nâš ï¸ æ²¡æœ‰æœ‰æ•ˆç»“æœ")
    
    print(f"\nç»“æœå·²ä¿å­˜è‡³: {args.out_dir}")
    timer.remove()


if __name__ == "__main__":
    main()
